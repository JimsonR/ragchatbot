{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09861d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def load_documents(folder_path):\n",
    "    docs = []\n",
    "    for file in Path(folder_path).glob(\"*.txt\"):\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            docs.append(f.read())\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5e44a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Load a sentence transformer model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def load_documents(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [f.read()]  # Return as a list with one item\n",
    "\n",
    "\n",
    "docs = load_documents(\"pg24440.txt\")\n",
    "chunks = [chunk for doc in docs for chunk in chunk_text(doc)]\n",
    "\n",
    "# Compute embeddings\n",
    "embeddings = embedding_model.encode(chunks, convert_to_numpy=True)\n",
    "\n",
    "# Build FAISS index only if embeddings are available\n",
    "if embeddings.shape[0] > 0 and len(embeddings.shape) > 1:\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(np.array(embeddings))\n",
    "else:\n",
    "    print(\"No embeddings to index. Please check your documents and chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7457d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=3):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    _, indices = index.search(np.array(query_embedding), k)\n",
    "    return [chunks[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c67212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\New folder (3)\\ragpractise\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jimmy\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "def generate_answer(query):\n",
    "    context = \"\\n\".join(retrieve(query))\n",
    "    prompt = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n",
    "    result = generator(prompt, max_length=200, do_sample=True)[0]['generated_text']\n",
    "    return result.split(\"Answer:\")[-1].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01dd9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: that is, for instance, or that it does not induce any of its\n",
      "\n",
      "changes, or also from a prolapse to\n",
      "\n",
      "the epididymis, and, if it does, or if so did\n",
      "\n",
      "be by itself and it cannot be\n",
      "\n",
      "cautiously\n",
      "\n",
      "came to be. This is more a matter of principle, I think, than any question\n",
      "\n",
      "of how or by whom it should be. For, I cannot get rid of an Aneurist\n"
     ]
    }
   ],
   "source": [
    "query = \"what is aneurism\"\n",
    "answer = generate_answer(query)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda6863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
